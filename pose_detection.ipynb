{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226\n"
     ]
    }
   ],
   "source": [
    "#Capturing frame and landmarks from data\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import csv\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Open your training video\n",
    "cap = cv2.VideoCapture('eyeblink3.avi')\n",
    "\n",
    "# Create a CSV file to store landmarks\n",
    "with open('pose_landmarks.csv', mode='a', newline='') as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    \n",
    "    # Define CSV header (add labels later)\n",
    "    headers = ['frame', 'label']\n",
    "    for i in range(33):  # MediaPipe Pose has 33 landmarks\n",
    "        headers += [f'x_{i}', f'y_{i}', f'z_{i}', f'visibility_{i}']\n",
    "    csv_writer.writerow(headers)\n",
    "\n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Process the frame to get pose landmarks\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(image_rgb)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            # Prepare row data for CSV\n",
    "            row = [frame_count, 'label_here']  # Use a placeholder label initially\n",
    "            \n",
    "            for landmark in results.pose_landmarks.landmark:\n",
    "                row.extend([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
    "            \n",
    "            # Write the row to CSV\n",
    "            csv_writer.writerow(row)\n",
    "        \n",
    "        frame_count += 1\n",
    "    print(frame_count)\n",
    "    cap.release()\n",
    "    pose.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV with pose landmarks and labels\n",
    "df = pd.read_csv('pose_landmarks.csv')\n",
    "\n",
    "# Normalize landmarks by the position of the left hip (landmark 23)\n",
    "for i in range(33):\n",
    "    df[f'x_{i}'] -= df['x_23']  # Normalizing by left hip x-coordinate\n",
    "    df[f'y_{i}'] -= df['y_23']  # Normalizing by left hip y-coordinate\n",
    "\n",
    "# Save the normalized data\n",
    "df.to_csv('pose_landmarks_normalized.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into Training and Testing Sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the normalized data\n",
    "df = pd.read_csv('pose_landmarks_normalized.csv')\n",
    "\n",
    "# Separate features (landmarks) and labels\n",
    "X = df.drop(columns=['frame', 'label'])\n",
    "y = df['label']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7356 - loss: 0.7658 - val_accuracy: 0.9291 - val_loss: 0.2676\n",
      "Epoch 2/20\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9607 - loss: 0.1682 - val_accuracy: 0.9515 - val_loss: 0.1503\n",
      "Epoch 3/20\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9712 - loss: 0.1100 - val_accuracy: 0.9590 - val_loss: 0.1240\n",
      "Epoch 4/20\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9849 - loss: 0.0736 - val_accuracy: 0.9552 - val_loss: 0.1195\n",
      "Epoch 5/20\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9768 - loss: 0.0816 - val_accuracy: 0.9627 - val_loss: 0.1053\n",
      "Epoch 6/20\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9865 - loss: 0.0459 - val_accuracy: 0.9627 - val_loss: 0.1003\n",
      "Epoch 7/20\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9885 - loss: 0.0370 - val_accuracy: 0.9627 - val_loss: 0.0977\n",
      "Epoch 8/20\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9882 - loss: 0.0414 - val_accuracy: 0.9739 - val_loss: 0.0871\n",
      "Epoch 9/20\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9886 - loss: 0.0380 - val_accuracy: 0.9776 - val_loss: 0.0791\n",
      "Epoch 10/20\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9912 - loss: 0.0310 - val_accuracy: 0.9739 - val_loss: 0.0778\n",
      "Epoch 11/20\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9902 - loss: 0.0313 - val_accuracy: 0.9813 - val_loss: 0.0792\n",
      "Epoch 12/20\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9918 - loss: 0.0304 - val_accuracy: 0.9739 - val_loss: 0.0784\n",
      "Epoch 13/20\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9959 - loss: 0.0224 - val_accuracy: 0.9776 - val_loss: 0.0656\n",
      "Epoch 14/20\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9942 - loss: 0.0279 - val_accuracy: 0.9776 - val_loss: 0.0681\n",
      "Epoch 15/20\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9968 - loss: 0.0156 - val_accuracy: 0.9813 - val_loss: 0.0594\n",
      "Epoch 16/20\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9946 - loss: 0.0221 - val_accuracy: 0.9851 - val_loss: 0.0576\n",
      "Epoch 17/20\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9956 - loss: 0.0162 - val_accuracy: 0.9813 - val_loss: 0.0636\n",
      "Epoch 18/20\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9946 - loss: 0.0176 - val_accuracy: 0.9813 - val_loss: 0.0519\n",
      "Epoch 19/20\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9964 - loss: 0.0132 - val_accuracy: 0.9851 - val_loss: 0.0493\n",
      "Epoch 20/20\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9954 - loss: 0.0140 - val_accuracy: 0.9888 - val_loss: 0.0495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to C:\\Users\\hp\\AppData\\Local\\Programs\\Microsoft VS Code\\my_model.h5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9861 - loss: 0.0624 \n",
      "Test accuracy: 0.9888059496879578\n"
     ]
    }
   ],
   "source": [
    "#Training a basic classifier using TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Load your labeled landmark data\n",
    "# X: the input features (pose landmarks), y: the pose labels\n",
    "# Assuming data is already in numpy format\n",
    "\n",
    "X = np.array(X, dtype='float32')  # Ensure the data is in numpy format and type float32\n",
    "y = np.array(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Define a simple neural network model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(set(y)), activation='softmax')  # Output layer\n",
    "])\n",
    "'''# Define a neural network model with the correct input shape \n",
    "model = tf.keras.models.Sequential([ \n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(99,)), \n",
    "    tf.keras.layers.Dense(32, activation='relu'), \n",
    "    tf.keras.layers.Dense(len(set(y)), activation='softmax') # Output layer \n",
    "])'''\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test))\n",
    "\n",
    "model_path = r'C:\\Users\\hp\\AppData\\Local\\Programs\\Microsoft VS Code\\my_model.h5' \n",
    "model.save(model_path) \n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of video or no frame to capture.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model(r'C:\\Users\\hp\\AppData\\Local\\Programs\\Microsoft VS Code\\my_model.h5')  # Make sure to provide the correct path to your model\n",
    "\n",
    "# Define label map (update with your specific labels)\n",
    "label_map = {0: \"smile\", 1: \"handshake\", 2: \"eyeblink\"}  # Adjust based on your model's output labels\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Open the video capture\n",
    "video_path = r'C:\\Users\\HP\\AppData\\Local\\Programs\\Microsoft VS Code\\eyeblink4.avi'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Use Pose with the proper confidence parameters\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"End of video or no frame to capture.\")\n",
    "            break\n",
    "        \n",
    "        # Convert BGR image to RGB for MediaPipe\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Process the image and detect pose landmarks\n",
    "        results = pose.process(image)\n",
    "\n",
    "        # Draw landmarks on the original BGR image\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        if results.pose_landmarks:\n",
    "            # Draw landmarks\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),\n",
    "                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "            )\n",
    "            \n",
    "            # Prepare landmarks for the model\n",
    "            landmarks = []\n",
    "            for lm in results.pose_landmarks.landmark:\n",
    "                landmarks.extend([lm.x, lm.y, lm.z, lm.visibility])  # Append each landmark as x, y, z, visibility\n",
    "\n",
    "            # Convert landmarks to numpy array and reshape for the model\n",
    "            landmarks = np.array(landmarks).reshape(1, -1)  # Shape (1, N) where N is the total landmarks data\n",
    "            \n",
    "            # Predict the pose\n",
    "            prediction = model.predict(landmarks)\n",
    "            predicted_label = np.argmax(prediction)  # Get the index of the highest probability\n",
    "            label_text = label_map[predicted_label]  # Map the index to a human-readable label\n",
    "            \n",
    "            # Display the label on the video\n",
    "            cv2.putText(image, f\"Pose: {label_text}\", (10, 40), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Display the image with the pose label\n",
    "        cv2.imshow(\"Pose Detection\", image)\n",
    "\n",
    "        # Exit with 'q' key\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
